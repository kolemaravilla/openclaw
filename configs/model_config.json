{
  "$schema": "https://open-claw.bot/schemas/openclaw-models.json",
  "_comment": [
    "Model provider configuration for Brock agent stack.",
    "Three tiers: local GLM-5 (fast/free), z.ai API (strong/coding), OpenAI (backup/reasoning).",
    "This file is a reference config — merge into your openclaw.json under 'models' and 'agents.defaults'."
  ],

  "models": {
    "mode": "merge",
    "providers": {

      "local": {
        "baseUrl": "http://localhost:11434/v1",
        "api": "ollama",
        "auth": "api-key",
        "apiKey": "",
        "headers": {},
        "models": [
          {
            "id": "glm-5",
            "name": "GLM-5 (local)",
            "reasoning": true,
            "input": ["text"],
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
            "contextWindow": 131072,
            "maxTokens": 8192,
            "compat": {}
          }
        ]
      },

      "zai": {
        "baseUrl": "https://api.z.ai/api/coding/paas/v4",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${ZAI_API_KEY}",
        "models": [
          {
            "id": "glm-5",
            "name": "GLM-5 (z.ai Coding)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.002, "output": 0.006, "cacheRead": 0.001, "cacheWrite": 0.002 },
            "contextWindow": 131072,
            "maxTokens": 16384
          },
          {
            "id": "glm-4.7",
            "name": "GLM-4.7 (z.ai Coding)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.001, "output": 0.003, "cacheRead": 0.0005, "cacheWrite": 0.001 },
            "contextWindow": 131072,
            "maxTokens": 16384
          }
        ]
      },

      "openai": {
        "baseUrl": "https://api.openai.com/v1",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${OPENAI_API_KEY}",
        "models": [
          {
            "id": "gpt-4o",
            "name": "GPT-4o (OpenAI backup)",
            "reasoning": false,
            "input": ["text", "image"],
            "cost": { "input": 0.0025, "output": 0.01, "cacheRead": 0.00125, "cacheWrite": 0.0025 },
            "contextWindow": 128000,
            "maxTokens": 16384
          },
          {
            "id": "o3",
            "name": "o3 (OpenAI reasoning)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.01, "output": 0.04, "cacheRead": 0.005, "cacheWrite": 0.01 },
            "contextWindow": 200000,
            "maxTokens": 100000
          }
        ]
      }
    }
  },

  "agents": {
    "defaults": {
      "model": {
        "primary": "zai/glm-5",
        "fallbacks": [
          "zai/glm-4.7",
          "openai/gpt-4o"
        ]
      },
      "imageModel": {
        "primary": "zai/glm-5",
        "fallbacks": ["openai/gpt-4o"]
      },
      "models": {
        "zai/glm-5": {
          "alias": "glm5",
          "streaming": true,
          "params": {}
        },
        "local/glm-5": {
          "alias": "local",
          "streaming": true,
          "params": {}
        },
        "openai/gpt-4o": {
          "alias": "gpt",
          "streaming": true,
          "params": {}
        },
        "openai/o3": {
          "alias": "o3",
          "streaming": true,
          "params": {}
        }
      }
    }
  },

  "_routing_notes": {
    "fast_tier": {
      "provider": "local/glm-5",
      "use_for": "Routine tasks: briefings, job filtering, formatting, quick lookups",
      "notes": [
        "Free, low latency, runs on your hardware",
        "Change baseUrl to match your local server (Ollama: 11434, vLLM: 8000, LM Studio: 1234)",
        "If running via vLLM, change api to 'openai-completions'",
        "GLM-5 is 744B params — needs significant GPU. Consider GLM-4.7 or a quantized variant if constrained"
      ]
    },
    "strong_tier": {
      "provider": "zai/glm-5",
      "use_for": "Code tasks, deep research, complex analysis, agentic workflows",
      "notes": [
        "Uses z.ai Coding Plan endpoint (cheaper than general API for coding tasks)",
        "Subscribe at z.ai console, get API key, set ZAI_API_KEY env var",
        "GLM-4.7 included as fallback — still strong, lower cost"
      ]
    },
    "backup_tier": {
      "provider": "openai/gpt-4o (general) or openai/o3 (reasoning)",
      "use_for": "Fallback when z.ai is down, or tasks that need different reasoning style",
      "notes": [
        "Set OPENAI_API_KEY env var",
        "o3 is expensive but strong for complex reasoning — use sparingly",
        "Can also add anthropic/claude-sonnet-4-6 here if you have an Anthropic key"
      ]
    }
  },

  "_soul_md_note": "SOUL.md was written for Claude. For local models (especially smaller ones), persona instructions may need to be more direct and repeated. If Brock drifts back to generic assistant behavior on local, try: (1) repeating key behavioral rules in the system prompt, (2) adding negative examples ('DO NOT say Great question'), (3) using shorter, more imperative phrasing. The z.ai GLM models tend to follow persona directives well."
}
