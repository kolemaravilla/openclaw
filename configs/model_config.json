{
  "$schema": "https://open-claw.bot/schemas/openclaw-models.json",
  "_comment": [
    "Model provider configuration for Brock agent stack.",
    "Four tiers: z.ai GLM-5 (code), DeepSeek (bulk), Kimi K2 (reasoning), OpenAI (fallback).",
    "Optional: local Ollama if hardware supports it.",
    "This file is a reference config — merge into your openclaw.json under 'models' and 'agents.defaults'.",
    "Full strategy rationale: see CLOUD_STRATEGY.md in the repo root."
  ],

  "models": {
    "mode": "merge",
    "providers": {

      "zai": {
        "baseUrl": "https://api.z.ai/api/coding/paas/v4",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${ZAI_API_KEY}",
        "models": [
          {
            "id": "glm-5",
            "name": "GLM-5 (z.ai Coding)",
            "reasoning": true,
            "input": ["text"],
            "cost": { "input": 1.0, "output": 3.2, "cacheRead": 0.2, "cacheWrite": 1.0 },
            "contextWindow": 200000,
            "maxTokens": 128000
          },
          {
            "id": "glm-4.7",
            "name": "GLM-4.7 (z.ai Coding)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.6, "output": 2.2, "cacheRead": 0.11, "cacheWrite": 0.6 },
            "contextWindow": 203000,
            "maxTokens": 16384
          },
          {
            "id": "glm-4.7-flash",
            "name": "GLM-4.7 Flash (z.ai, free)",
            "reasoning": false,
            "input": ["text"],
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
            "contextWindow": 203000,
            "maxTokens": 16384,
            "_comment": "Free tier model — good for ultra-cheap fast work"
          }
        ]
      },

      "deepseek": {
        "baseUrl": "https://api.deepseek.com/v1",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${DEEPSEEK_API_KEY}",
        "models": [
          {
            "id": "deepseek-chat",
            "name": "DeepSeek V3.2 Chat (bulk)",
            "reasoning": false,
            "input": ["text"],
            "cost": { "input": 0.28, "output": 0.42, "cacheRead": 0.028, "cacheWrite": 0.28 },
            "contextWindow": 128000,
            "maxTokens": 8192
          },
          {
            "id": "deepseek-reasoner",
            "name": "DeepSeek V3.2 Reasoner (thinking)",
            "reasoning": true,
            "input": ["text"],
            "cost": { "input": 0.28, "output": 0.42, "cacheRead": 0.028, "cacheWrite": 0.28 },
            "contextWindow": 128000,
            "maxTokens": 65536
          }
        ]
      },

      "moonshot": {
        "baseUrl": "https://api.moonshot.ai/v1",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${MOONSHOT_API_KEY}",
        "models": [
          {
            "id": "kimi-k2-thinking",
            "name": "Kimi K2 Thinking (reasoning)",
            "reasoning": true,
            "input": ["text"],
            "cost": { "input": 0.6, "output": 2.5, "cacheRead": 0.15, "cacheWrite": 0.6 },
            "contextWindow": 256000,
            "maxTokens": 65536
          },
          {
            "id": "kimi-k2-thinking-turbo",
            "name": "Kimi K2 Thinking Turbo (fast reasoning)",
            "reasoning": true,
            "input": ["text"],
            "cost": { "input": 1.15, "output": 8.0, "cacheRead": 0.29, "cacheWrite": 1.15 },
            "contextWindow": 256000,
            "maxTokens": 65536,
            "_comment": "Up to 100 tok/s. Use when reasoning speed matters."
          },
          {
            "id": "kimi-k2",
            "name": "Kimi K2 Base (fast, non-thinking)",
            "reasoning": false,
            "input": ["text"],
            "cost": { "input": 0.6, "output": 2.5, "cacheRead": 0.15, "cacheWrite": 0.6 },
            "contextWindow": 256000,
            "maxTokens": 32768
          }
        ]
      },

      "openai": {
        "baseUrl": "https://api.openai.com/v1",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${OPENAI_API_KEY}",
        "models": [
          {
            "id": "gpt-4o",
            "name": "GPT-4o (OpenAI fallback)",
            "reasoning": false,
            "input": ["text", "image"],
            "cost": { "input": 2.5, "output": 10.0, "cacheRead": 1.25, "cacheWrite": 2.5 },
            "contextWindow": 128000,
            "maxTokens": 16384
          },
          {
            "id": "o3",
            "name": "o3 (OpenAI reasoning fallback)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 10.0, "output": 40.0, "cacheRead": 5.0, "cacheWrite": 10.0 },
            "contextWindow": 200000,
            "maxTokens": 100000
          }
        ]
      },

      "local": {
        "baseUrl": "http://localhost:11434/v1",
        "api": "ollama",
        "auth": "api-key",
        "apiKey": "",
        "headers": {},
        "_comment": "Optional local Ollama provider. Pull whichever model fits your hardware.",
        "models": [
          {
            "id": "glm-5",
            "name": "GLM-5 (local, quantized)",
            "reasoning": true,
            "input": ["text"],
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
            "contextWindow": 131072,
            "maxTokens": 8192,
            "compat": {}
          }
        ]
      }
    }
  },

  "agents": {
    "defaults": {
      "model": {
        "primary": "zai/glm-5",
        "fallbacks": [
          "moonshot/kimi-k2-thinking",
          "deepseek/deepseek-chat",
          "openai/gpt-4o"
        ]
      },
      "bulkModel": {
        "primary": "deepseek/deepseek-chat",
        "fallbacks": [
          "moonshot/kimi-k2-thinking",
          "openai/gpt-4o"
        ]
      },
      "reasoningModel": {
        "primary": "moonshot/kimi-k2-thinking",
        "fallbacks": [
          "zai/glm-5",
          "openai/o3"
        ]
      },
      "imageModel": {
        "primary": "openai/gpt-4o",
        "fallbacks": ["openai/o3"]
      },
      "models": {
        "zai/glm-5": {
          "alias": "glm5",
          "streaming": true,
          "params": {}
        },
        "deepseek/deepseek-chat": {
          "alias": "ds",
          "streaming": true,
          "params": {}
        },
        "deepseek/deepseek-reasoner": {
          "alias": "dsr",
          "streaming": true,
          "params": {}
        },
        "moonshot/kimi-k2-thinking": {
          "alias": "kimi",
          "streaming": true,
          "params": {}
        },
        "moonshot/kimi-k2-thinking-turbo": {
          "alias": "kimi-turbo",
          "streaming": true,
          "params": {}
        },
        "moonshot/kimi-k2": {
          "alias": "kimi-base",
          "streaming": true,
          "params": {}
        },
        "openai/gpt-4o": {
          "alias": "gpt",
          "streaming": true,
          "params": {}
        },
        "openai/o3": {
          "alias": "o3",
          "streaming": true,
          "params": {}
        },
        "local/glm-5": {
          "alias": "local",
          "streaming": true,
          "params": {}
        }
      }
    }
  },

  "_routing_notes": "See CLOUD_STRATEGY.md for full tier descriptions, failover logic, cost analysis, and the openclaw/lobsterBucket boundary.",

  "_soul_md_note": "SOUL.md was written for Claude. For local models (especially smaller ones), persona instructions may need to be more direct and repeated. If Brock drifts back to generic assistant behavior on local, try: (1) repeating key behavioral rules in the system prompt, (2) adding negative examples ('DO NOT say Great question'), (3) using shorter, more imperative phrasing. The z.ai GLM models and Kimi K2 tend to follow persona directives well."
}
